{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c453cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0578dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --force-reinstall pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76348576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df48d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for regression\n",
    "def read_dataframe(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "    #calculate duratiion time in minutes and storatinfg it in the vairabale \"Duration\"\n",
    "\n",
    "    df['Duration'] = df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']\n",
    "    df['Duration'] = df['Duration'].apply(lambda x: x.total_seconds() / 60)\n",
    "    df1 = df[((df.Duration > 1) & (df.Duration <= 60))]\n",
    "    \n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    numerical = ['trip_distance']\n",
    "\n",
    "    # Create a DataFrame df2 containing selected columns from df1\n",
    "    df2 = df1[categorical + numerical].copy()\n",
    "\n",
    "    # Add a new column 'Duration' from df1 to df2\n",
    "    df2['Duration'] = df1['Duration']\n",
    "    \n",
    "    # Drop the target column from the dataframe\n",
    "    X = df2.drop(columns=['Duration'], axis=1)\n",
    "\n",
    "    # Get the target variable\n",
    "    y = df2['Duration']\n",
    "    \n",
    "    categorical_columns = ['PULocationID', 'DOLocationID']  # List of categorical columns\n",
    "\n",
    "    # Initialize OneHotEncoder\n",
    "    one_hot_encoder = OneHotEncoder(sparse=True)\n",
    "\n",
    "\n",
    "    # Fit and transform the categorical columns\n",
    "    df_encoded = one_hot_encoder.fit_transform(df2[categorical_columns])\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_encoded, df2['Duration'], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize the linear regression model\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train, y_train)\n",
    "        \n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    \n",
    "    return [\"Mean Squared Error (test): \" + str(mse)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f574a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 7.65070954759519\n",
      "Mean Squared Error: 7.783827641570997\n"
     ]
    }
   ],
   "source": [
    "train= read_dataframe('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet')\n",
    "validate= read_dataframe('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2819359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "    shape = df.shape\n",
    "    return df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e04984c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3066766, 19)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b838c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "    df['Duration'] = df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']\n",
    "    df['Duration'] = df['Duration'].apply(lambda x: x.total_seconds() / 60)\n",
    "    df1 = df[((df.Duration > 1) & (df.Duration <= 60))]\n",
    "    std = np.std(df.Duration)\n",
    "    return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3be27104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.5943442974141"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba73574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for lasso regression\n",
    "def read_dataframe(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "    # Calculate duration time in minutes and store it in the variable \"Duration\"\n",
    "    df['Duration'] = df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']\n",
    "    df['Duration'] = df['Duration'].apply(lambda x: x.total_seconds() / 60)\n",
    "    df1 = df[((df.Duration > 1) & (df.Duration <= 60))]\n",
    "    \n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    numerical = ['trip_distance']\n",
    "\n",
    "    # Create a DataFrame df2 containing selected columns from df1\n",
    "    df2 = df1[categorical + numerical].copy()\n",
    "\n",
    "    # Add a new column 'Duration' from df1 to df2\n",
    "    df2['Duration'] = df1['Duration']\n",
    "    \n",
    "    # Drop the target column from the dataframe\n",
    "    X = df2.drop(columns=['Duration'], axis=1)\n",
    "\n",
    "    # Get the target variable \n",
    "    y = df2['Duration']\n",
    "    \n",
    "    categorical_columns = ['PULocationID', 'DOLocationID']  # List of categorical columns\n",
    "\n",
    "    # Initialize OneHotEncoder\n",
    "    one_hot_encoder = OneHotEncoder(sparse=True)\n",
    "\n",
    "    # Fit and transform the categorical columns\n",
    "    df_encoded = one_hot_encoder.fit_transform(df2[categorical_columns])\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_encoded, df2['Duration'], test_size=0.2, random_state=42)\n",
    "\n",
    "    alphas = [0.01, 0.001, 0.00001, 0.05]\n",
    "    results = []\n",
    "\n",
    "    for alpha in alphas:\n",
    "        # Fit the Lasso model to your training data\n",
    "        lasso_model = Lasso(alpha=alpha)\n",
    "        lasso_model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict the target variable for your test data\n",
    "        y_pred = lasso_model.predict(X_test)\n",
    "\n",
    "        # Calculate mean squared error\n",
    "        mse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        print(f\"Mean Squared Error for alpha={alpha}: {mse}\")\n",
    "        results.append((alpha, mse))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c318e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for alpha=0.01: 8.017330516339682\n",
      "Mean Squared Error for alpha=0.001: 7.718834776863121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:609: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 40961536.3582398, tolerance: 23756.37349298848\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for alpha=1e-05: 7.650620611548999\n",
      "Mean Squared Error for alpha=0.05: 8.451714672670622\n",
      "Mean Squared Error for alpha=0.01: 8.117408601162468\n",
      "Mean Squared Error for alpha=0.001: 7.853034292340794\n"
     ]
    }
   ],
   "source": [
    "train= read_dataframe('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet')\n",
    "validate= read_dataframe('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc4eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# function for lasso regression with gridsearch\n",
    "\n",
    "def read_dataframe(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "    # Calculate duration time in minutes and store it in the variable \"Duration\"\n",
    "    df['Duration'] = df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']\n",
    "    df['Duration'] = df['Duration'].apply(lambda x: x.total_seconds() / 60)\n",
    "    df1 = df[((df.Duration > 1) & (df.Duration <= 60))]\n",
    "    \n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    numerical = ['trip_distance']\n",
    "\n",
    "    # Create a DataFrame df2 containing selected columns from df1\n",
    "    df2 = df1[categorical + numerical].copy()\n",
    "\n",
    "    # Add a new column 'Duration' from df1 to df2\n",
    "    df2['Duration'] = df1['Duration']\n",
    "    \n",
    "    # Drop the target column from the dataframe\n",
    "    X = df2.drop(columns=['Duration'], axis=1)\n",
    "\n",
    "    # Get the target variable \n",
    "    y = df2['Duration']\n",
    "    \n",
    "    categorical_columns = ['PULocationID', 'DOLocationID']  # List of categorical columns\n",
    "\n",
    "    # Initialize OneHotEncoder\n",
    "    one_hot_encoder = OneHotEncoder(sparse=True)\n",
    "\n",
    "    # Fit and transform the categorical columns\n",
    "    df_encoded = one_hot_encoder.fit_transform(df2[categorical_columns])\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_encoded, df2['Duration'], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the hyperparameters grid\n",
    "    param_grid = {'alpha': [0.1, 0.01,  0.0001, 0.000001, 0.5, 5, 1.0, 10.0, 50.0, 100.0, 500.0, 1000.0]}\n",
    "\n",
    "    # Initialize the Lasso model\n",
    "    lasso_model = Lasso()\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=lasso_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "\n",
    "    # Fit GridSearchCV to the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predict the target variable for your test data using the best model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    mse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "    return grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a853c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "train= read_dataframe('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet')\n",
    "validate= read_dataframe('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76233d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''param_grid = {'alpha': [0.1, 0.01,  0.0001, 0.000001, 0.5, 5, 1.0, 10.0, 50.0, 100.0, 500.0, 1000.0]}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bccbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "'''with open('models/lin_reg/bin', 'wb') as f_out:\n",
    "    pickle.dump((one_hot_encoder, model), f_out)\n",
    "    \n",
    "    with open('models/lin_reg/bin', 'wb') as f_out:\n",
    "        pickle.dump((one_hot_encoder, lasso_model), f_out)\n",
    "        \n",
    "        \n",
    "        with open('models/lin_reg/bin', 'wb') as f_out:\n",
    "        pickle.dump((one_hot_encoder, best_model), f_out)\n",
    "    \n",
    "    return grid_search.best_params_'''\n",
    "    \n",
    "    \n",
    "# Ensure the directory exists\n",
    "    model_dir = 'models/grid_search'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the OneHotEncoder and the best model using pickle\n",
    "with open(os.path.join(model_dir, 'bin'), 'wb') as f_out:\n",
    "    pickle.dump((one_hot_encoder, best_model), f_out)\n",
    "    \n",
    "print(\"Models have been saved to model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9956e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   \n",
    "# Ensure the directory exists\n",
    "model_dir = 'models/lasso_reg'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "with open(os.path.join(model_dir, 'bin'),'wb') as f_out:\n",
    "        pickle.dump((one_hot_encoder, lasso_model), f_out)\n",
    "\n",
    "print(\"Models have been saved to model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ff96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Ensure the directory exists\n",
    "model_dir = 'models/lin_reg'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "with open(os.path.join(model_dir, 'bin'),'wb') as f_out:\n",
    "        pickle.dump((one_hot_encoder, model), f_out)\n",
    "print(\"Model have been saved to model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b039675",
   "metadata": {},
   "outputs": [],
   "source": [
    "19, [], 98%, 2, 7.64, 7.81, {}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
